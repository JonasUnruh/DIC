{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f28b29-a5ed-4fde-8842-7b7884de80ea",
   "metadata": {},
   "source": [
    "## Part 2 Datasets/DataFrames: Spark ML and Pipelines\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation pipeline. The primary goal of this part is the preparation of the pipeline for Part 3 (see below). Note: although parts of this pipeline will be very similar to Assignment 1 or Part 1 above, do not expect to obtain identical results or have access to all intermediate outputs to compare the individual steps.\n",
    "\n",
    "Use built-in functions for tokenization to unigrams at whitespaces, tabs, digits, and the delimiter characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/, casefolding, stopword removal, TF-IDF calculation, and chi square selection ) (using 2000 top terms overall). Write the terms selected this way to a file output_ds.txt and compare them with the terms selected in Assignment 1. Describe your observations briefly in the submission report (see Part 3).\n",
    "\n",
    "[Provided link for ML pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html)  \n",
    "[Provided link for feature extraction](https://spark.apache.org/docs/latest/ml-features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8870c8-86f0-4b27-bb23-897de467d592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473068a3-f3cf-43f7-853b-caaede739ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/14 12:11:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/14 12:11:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/14 12:11:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/14 12:11:25 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/14 12:11:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47068)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57110)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35974)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44226)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60772)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52416)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47510)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54520)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44374)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43300)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47456)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51812)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59224)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41728)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39884)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41304)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57470)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51588)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37286)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52978)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60118)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34178)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36690)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54210)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56748)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41018)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57264)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56526)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36706)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark context and session\n",
    "conf = SparkConf().setAppName(\"Part2\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b857ce-e484-4d92-991e-0b3c71d1ef7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter01.os.hpc.tuwien.ac.at:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Part2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ff42c5a00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abadc07e-77eb-48cb-8b6f-f444c7362aa1",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Stopwords file as well as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee8fe45-dfdb-40c4-b0b6-727c45252da3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the stopwords file and the counters file\n",
    "stopwords_file = \"stopwords.txt\"\n",
    "\n",
    "# Load stopwords into a set\n",
    "with open(stopwords_file, \"r\") as f:\n",
    "    stopwords = set(f.read().strip().split())\n",
    "    \n",
    "# Load and preprocess the Amazon reviews dataset\n",
    "input_file = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "reviews_df = spark.read.json(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d527f40f-3aa7-4e2e-84c4-c0761ddb096e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...|\n",
      "|Patio_Lawn_and_Garde|This product is a...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df = reviews_df.select(\"category\", \"reviewText\")\n",
    "\n",
    "# Show the DataFrame with selected columns\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723b5b92-c7a9-4453-bb72-e92a3f7521e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='Patio_Lawn_and_Garde', reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\"),\n",
       " Row(category='Patio_Lawn_and_Garde', reviewText='This is a very nice spreader.  It feels very solid and the pneumatic tires give it great maneuverability and handling over bumps.  The control arm is solid metal, not a cable, which gives you precise control and will last a long time.  The settings take some experimentation with your various products to get it right, but that is true of any spreader.  It has good distribution... probably flings material a little farther on the right side than the left, but it is far more even than my crappy Edgeguard ever was.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first two objects of reviews_rdd\n",
    "reviews_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018993a2-4daa-4020-a15a-1f755d9e97f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = r'\\s+|\\d+|[(){}\\[\\].!?,;:+=_\"\\'`~#@&*%€$§\\\\/\\-]'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a307cd-bf0f-49fb-8915-36cf8312b232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_df = regexTokenizer.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcafa32-7d27-4a63-ab95-bcd9b56185e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category', 'reviewText', 'words']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714cafaf-b8e6-423f-b60b-3f9ed1007950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(category='Patio_Lawn_and_Garde', reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", words=['this', 'was', 'a', 'gift', 'for', 'my', 'other', 'husband', 'he', 's', 'making', 'us', 'things', 'from', 'it', 'all', 'the', 'time', 'and', 'we', 'love', 'the', 'food', 'directions', 'are', 'simple', 'easy', 'to', 'read', 'and', 'interpret', 'and', 'fun', 'to', 'make', 'we', 'all', 'love', 'different', 'kinds', 'of', 'cuisine', 'and', 'raichlen', 'provides', 'recipes', 'from', 'everywhere', 'along', 'the', 'barbecue', 'trail', 'as', 'he', 'calls', 'it', 'get', 'it', 'and', 'just', 'open', 'a', 'page', 'have', 'at', 'it', 'you', 'll', 'love', 'the', 'food', 'and', 'it', 'has', 'provided', 'us', 'with', 'an', 'insight', 'into', 'the', 'culture', 'that', 'produced', 'it', 'it', 's', 'all', 'about', 'broadening', 'horizons', 'yum'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ddf7ecb-9a38-4923-a7e7-de238ad7a47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a935c1f3-84b4-478f-a6fb-b4718df9e7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(stopWords = list(stopwords), inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "reviews_df = remover.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b41b9-f418-438b-8be6-ffd2cb5882df",
   "metadata": {},
   "source": [
    "Add HashingTF HashingTermFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51d02b4-d9b5-444a-982a-a35605ea1440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "reviews_df = hashingTF.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8083922-f1c8-43a2-aaec-333c53162eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='Patio_Lawn_and_Garde', reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", words=['this', 'was', 'a', 'gift', 'for', 'my', 'other', 'husband', 'he', 's', 'making', 'us', 'things', 'from', 'it', 'all', 'the', 'time', 'and', 'we', 'love', 'the', 'food', 'directions', 'are', 'simple', 'easy', 'to', 'read', 'and', 'interpret', 'and', 'fun', 'to', 'make', 'we', 'all', 'love', 'different', 'kinds', 'of', 'cuisine', 'and', 'raichlen', 'provides', 'recipes', 'from', 'everywhere', 'along', 'the', 'barbecue', 'trail', 'as', 'he', 'calls', 'it', 'get', 'it', 'and', 'just', 'open', 'a', 'page', 'have', 'at', 'it', 'you', 'll', 'love', 'the', 'food', 'and', 'it', 'has', 'provided', 'us', 'with', 'an', 'insight', 'into', 'the', 'culture', 'that', 'produced', 'it', 'it', 's', 'all', 'about', 'broadening', 'horizons', 'yum'], filtered_words=['gift', 'husband', 'making', 'things', 'time', 'love', 'food', 'directions', 'simple', 'easy', 'interpret', 'make', 'love', 'kinds', 'cuisine', 'raichlen', 'recipes', 'barbecue', 'trail', 'calls', 'open', 'page', 'love', 'food', 'provided', 'insight', 'culture', 'produced', 'broadening', 'horizons', 'yum'], rawFeatures=SparseVector(2000, {157: 1.0, 240: 3.0, 358: 1.0, 391: 1.0, 436: 1.0, 483: 1.0, 628: 1.0, 645: 1.0, 653: 2.0, 731: 1.0, 760: 1.0, 820: 1.0, 827: 1.0, 913: 1.0, 1061: 1.0, 1063: 1.0, 1080: 1.0, 1198: 1.0, 1208: 1.0, 1386: 1.0, 1416: 1.0, 1457: 1.0, 1460: 1.0, 1522: 1.0, 1525: 1.0, 1636: 1.0, 1783: 1.0, 1853: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939ee103-64d4-4175-8e00-32352ee5d453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(reviews_df)\n",
    "reviews_df = idfModel.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d75cd2-a1bf-4fe1-9c78-9a5216677dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='Patio_Lawn_and_Garde', reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", words=['this', 'was', 'a', 'gift', 'for', 'my', 'other', 'husband', 'he', 's', 'making', 'us', 'things', 'from', 'it', 'all', 'the', 'time', 'and', 'we', 'love', 'the', 'food', 'directions', 'are', 'simple', 'easy', 'to', 'read', 'and', 'interpret', 'and', 'fun', 'to', 'make', 'we', 'all', 'love', 'different', 'kinds', 'of', 'cuisine', 'and', 'raichlen', 'provides', 'recipes', 'from', 'everywhere', 'along', 'the', 'barbecue', 'trail', 'as', 'he', 'calls', 'it', 'get', 'it', 'and', 'just', 'open', 'a', 'page', 'have', 'at', 'it', 'you', 'll', 'love', 'the', 'food', 'and', 'it', 'has', 'provided', 'us', 'with', 'an', 'insight', 'into', 'the', 'culture', 'that', 'produced', 'it', 'it', 's', 'all', 'about', 'broadening', 'horizons', 'yum'], filtered_words=['gift', 'husband', 'making', 'things', 'time', 'love', 'food', 'directions', 'simple', 'easy', 'interpret', 'make', 'love', 'kinds', 'cuisine', 'raichlen', 'recipes', 'barbecue', 'trail', 'calls', 'open', 'page', 'love', 'food', 'provided', 'insight', 'culture', 'produced', 'broadening', 'horizons', 'yum'], rawFeatures=SparseVector(2000, {157: 1.0, 240: 3.0, 358: 1.0, 391: 1.0, 436: 1.0, 483: 1.0, 628: 1.0, 645: 1.0, 653: 2.0, 731: 1.0, 760: 1.0, 820: 1.0, 827: 1.0, 913: 1.0, 1061: 1.0, 1063: 1.0, 1080: 1.0, 1198: 1.0, 1208: 1.0, 1386: 1.0, 1416: 1.0, 1457: 1.0, 1460: 1.0, 1522: 1.0, 1525: 1.0, 1636: 1.0, 1783: 1.0, 1853: 1.0}), features=SparseVector(2000, {157: 1.8002, 240: 5.5305, 358: 3.5048, 391: 4.3281, 436: 4.2223, 483: 4.8309, 628: 4.442, 645: 4.3426, 653: 8.0347, 731: 3.8665, 760: 2.1829, 820: 5.4373, 827: 3.6756, 913: 2.4104, 1061: 3.5133, 1063: 6.3551, 1080: 4.6523, 1198: 4.2363, 1208: 3.2237, 1386: 5.0289, 1416: 4.5694, 1457: 4.5805, 1460: 3.0276, 1522: 4.9189, 1525: 2.4746, 1636: 4.5954, 1783: 2.774, 1853: 2.7371}))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c3fcc-5a87-4ed9-9a94-76e373521982",
   "metadata": {},
   "source": [
    "To implement the Chi Square selector, we first need to convert the categories into numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623e43a1-b476-459a-820a-328b88c5cea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply StringIndexer to convert categorical column to numerical\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "reviews_df = indexer.fit(reviews_df).transform(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b2ea2e-c079-4812-9e44-0126728e1ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|categoryIndex|\n",
      "+-------------+\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "|         18.0|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.select('categoryIndex').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6e57f-31aa-4fd2-8030-2d59768169f4",
   "metadata": {},
   "source": [
    "## TODO\n",
    "Since ChiSquare Selector does not calculate the Chi square value by default but right now only selects the top n values, we need to figure out if there is a function for it or if we have to write it manually as done in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc18d5b-0676-400d-b052-7bef239683fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=75, featuresCol=\"features\",outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\")\n",
    "result = selector.fit(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da257682-1594-450c-b18f-29b3a328bf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
