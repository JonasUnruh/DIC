{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f28b29-a5ed-4fde-8842-7b7884de80ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Part 2 Datasets/DataFrames: Spark ML and Pipelines\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation pipeline. The primary goal of this part is the preparation of the pipeline for Part 3 (see below). Note: although parts of this pipeline will be very similar to Assignment 1 or Part 1 above, do not expect to obtain identical results or have access to all intermediate outputs to compare the individual steps.\n",
    "\n",
    "Use built-in functions for tokenization to unigrams at whitespaces, tabs, digits, and the delimiter characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/, casefolding, stopword removal, TF-IDF calculation, and chi square selection ) (using 2000 top terms overall). Write the terms selected this way to a file output_ds.txt and compare them with the terms selected in Assignment 1. Describe your observations briefly in the submission report (see Part 3).\n",
    "\n",
    "[Provided link for ML pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html)  \n",
    "[Provided link for feature extraction](https://spark.apache.org/docs/latest/ml-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c5014-586e-4207-a1ee-4ae70578a208",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8870c8-86f0-4b27-bb23-897de467d592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import UnivariateFeatureSelector, ChiSqSelectorModel \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338b89d-b829-47f2-b8a4-41237bf548e0",
   "metadata": {},
   "source": [
    "## Initiliaze Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473068a3-f3cf-43f7-853b-caaede739ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/17 14:51:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/17 14:51:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/17 14:51:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/17 14:51:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/17 14:51:48 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/05/17 14:51:48 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/05/17 14:51:50 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark context and session\n",
    "conf = SparkConf().setAppName(\"Part2\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b857ce-e484-4d92-991e-0b3c71d1ef7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter01.os.hpc.tuwien.ac.at:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Part2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdc8009e6a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abadc07e-77eb-48cb-8b6f-f444c7362aa1",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Stopwords file as well as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee8fe45-dfdb-40c4-b0b6-727c45252da3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the stopwords file and the counters file\n",
    "stopwords_file = \"stopwords.txt\"\n",
    "\n",
    "# Load stopwords into a set\n",
    "with open(stopwords_file, \"r\") as f:\n",
    "    stopwords = set(f.read().strip().split())\n",
    "    \n",
    "# Load and preprocess the Amazon reviews dataset\n",
    "input_file = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "reviews_df = spark.read.json(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d527f40f-3aa7-4e2e-84c4-c0761ddb096e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...|\n",
      "|Patio_Lawn_and_Garde|This product is a...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df = reviews_df.select(\"category\", \"reviewText\")\n",
    "\n",
    "# Show the DataFrame with selected columns\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a960f-e270-441c-8ac3-459f7651f821",
   "metadata": {},
   "source": [
    "## Create RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018993a2-4daa-4020-a15a-1f755d9e97f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = r'\\s+|\\d+|[(){}\\[\\].!?,;:+=_\"\\'`~#@&*%€$§\\\\/\\-]'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24675b04-1bd4-434d-81d7-4d021541c386",
   "metadata": {},
   "source": [
    "## Create StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a935c1f3-84b4-478f-a6fb-b4718df9e7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(stopWords = list(stopwords), inputCol=\"words\", outputCol=\"filtered_words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f995c64-2f56-43ce-8361-053506eff072",
   "metadata": {},
   "source": [
    "## Create CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51d02b4-d9b5-444a-982a-a35605ea1440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
    "countV = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae8be-1adb-4678-8734-9a58adbaea36",
   "metadata": {},
   "source": [
    "## Create IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939ee103-64d4-4175-8e00-32352ee5d453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c3fcc-5a87-4ed9-9a94-76e373521982",
   "metadata": {},
   "source": [
    "## Create StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623e43a1-b476-459a-820a-328b88c5cea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply StringIndexer to convert categorical column to numerical\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ab712-7d3f-41d6-a524-a44c7fddf2ac",
   "metadata": {},
   "source": [
    "## Create Selector\n",
    "Here, we add a SelectionThreshold of 2000 to have the top 2000 terms overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc18d5b-0676-400d-b052-7bef239683fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnivariateFeatureSelector_156a50f66587"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"features\",outputCol=\"selectedFeatures\",\n",
    "    labelCol=\"categoryIndex\",  selectionMode=\"numTopFeatures\",\n",
    ")\n",
    "selector.setFeatureType(\"categorical\").setLabelType(\"categorical\").setSelectionThreshold(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384dbc-28c3-4d5f-9cad-8c1ef079c20f",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da257682-1594-450c-b18f-29b3a328bf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, remover, countV, idf, indexer, selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d434b0f7-6550-44fc-8646-f03ef8d590a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 14:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1059.7 KiB\n",
      "24/05/17 14:54:54 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/17 14:54:54 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/17 14:55:01 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2bcade8-b9d6-4f68-8da1-7f0df3981f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = model.transform(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf8864-ee28-4c1e-8385-7755946fabee",
   "metadata": {},
   "source": [
    "## Get top 2000 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fbaffe6-c201-4c37-bea8-cca0414943cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract vocabulary from CountVectorizer\n",
    "vocabulary = model.stages[2].vocabulary\n",
    "\n",
    "# Map selected feature indices to terms\n",
    "selected_terms = [vocabulary[i] for i in model.stages[-1].selectedFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17a0630f-b8aa-4486-bc8f-02879a6439f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "796e0a6e-2af6-4bfe-a982-517b0074d973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:30] == selected_terms[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f65c8969-7816-4a5a-aed4-9c58fc81d2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(selected_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b1e266b-6ec9-44e3-afa8-e70e028c01f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    for term in selected_terms:\n",
    "        f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f6be8-c515-4b39-81dc-2b154e03d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
